# -*- coding: utf-8 -*-
"""plots for funnel.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1Fp6pzsiFSgizAi3MUesA3WwAHJpm0KCg
"""

import math
import numpy as np
epi_len = 10
h = 0.01
xd=[[-1.5+5.8*math.cos(0.24*h*t+1.5), 3*math.sin(0.24*t*h+1.5)] for t in range(epi_len)]
state_d = np.array(xd)

import matplotlib.pyplot as plt
plt.plot(state_d[:,0],state_d[:,1])

x = -3.19
y = 1.70
theta = 0
time_int = h
xd=[[-1.5+5.8*math.cos(0.24*time_int*t+1.5), 5.8*math.sin(0.24*t*time_int+1.5)] for t in range(epi_len)]
state_d = np.array(xd)
l = np.array([0.7,0.7])
rho_f = np.array([0.2,0.2])
rho_0 = np.array(abs(np.array([x,y])- state_d[0,:]) + 0.2)
gamma = np.array([(rho_0 - rho_f)*np.exp(-l*time_int*t) + rho_f for t in range(epi_len)])
gamma_x = np.array([(rho_0[0] - rho_f[0])*np.exp(-l[0]*time_int*t) + rho_f[0] for t in range(epi_len)])
gamma_y = np.array([(rho_0[1] - rho_f[1])*np.exp(-l[1]*time_int*t) + rho_f[1] for t in range(epi_len)])
lb_soft_x = state_d[:,0] - gamma_x
lb_soft_y = state_d[:,1] - gamma_y
ub_soft_x = state_d[:,0] + gamma_x
ub_soft_y = state_d[:,1] + gamma_y
lb_soft = state_d - gamma
ub_soft = state_d + gamma
phi_ini_L = [0,0]
phi_ini_U = [0,0]

time = np.linspace(0,h*epi_len,epi_len)
plt.plot(time,lb_soft[:,0],time,ub_soft[:,0])
plt.grid()

plt.plot(time,lb_soft[:,1],time,ub_soft[:,1])
plt.grid()

from scipy.integrate import odeint

def bound(phi,t,lb, ub, mu, kc):
    eta = ub-lb
    dphi_dt = 0.5*(1-np.sign(eta-mu))*(1/(eta+phi))-kc*phi
    return dphi_dt
  #    --------ADD FUNNEL HERE---------
def funnel(lb_soft, ub_soft, phi_ini_L, phi_ini_U):
    mu = np.array([5,5])   #For different states
    kc = np.array([5.0, 5.0])   #For different states
    # Hard Constraints - Boundary
    lb_hard = np.array([-6.58, -4.63])
    ub_hard = np.array([6.58, 4.63])

    # Soft Constraints - Trajectory tracking-as input
    t1 = [0,h]
    t, phi_L = odeint(bound, phi_ini_L, t1, args=(lb_soft, ub_hard, mu, kc))
    phi_sol_L = phi_L #np.abs(phi_L)
    t, phi_U = odeint(bound, phi_ini_U, t1, args=(lb_hard, ub_soft, mu, kc))
    phi_sol_U = phi_U #np.abs(phi_U)
    v = 10
    Lb = np.log(np.exp(v * (lb_soft - phi_sol_L)) + np.exp(v * lb_hard)) / v
    Ub = -np.log(np.exp(-v * (ub_soft + phi_sol_U)) + np.exp(-v * ub_hard)) / v
    return phi_sol_L, phi_sol_U, Lb, Ub

low_bound,upper_bound,time = [],[],[]
for j in range(epi_len):
  phi_sol_L,phi_sol_U,Lb,Ub = funnel(lb_soft[j,:],ub_soft[j,:],np.array(phi_ini_L),np.array(phi_ini_U))
  phi_ini_L = phi_sol_L
  phi_ini_U = phi_sol_U
  low_bound.append(Lb)
  upper_bound.append(Ub)
  time.append(j*h)

low_x = [inner_list[0] for inner_list in low_bound]
low_y = [inner_list[1] for inner_list in low_bound]
high_x = [inner_list[0] for inner_list in upper_bound]
high_y = [inner_list[1] for inner_list in upper_bound]

lb_hard = [-6.58 for i in range(epi_len)]
ub_hard = [6.58 for i in range(epi_len)]

plt.plot(time,low_x,time,high_x,time, lb_hard,time,ub_hard)
plt.grid()

lb_hard_y = [-4.63 for i in range(epi_len)]
ub_hard_y = [4.63 for i in range(epi_len)]
plt.plot(time,low_y,time,high_y,time,lb_hard_y,time,ub_hard_y)
plt.grid()

pip install gymnasium

import numpy as np
import math
import gymnasium as gym
from gymnasium.spaces import Box
from scipy.integrate import odeint
import matplotlib.pyplot as plt
class RobotEnv(gym.Env):
    def __init__(self):
        """
        Must define self.acion_space and self.observation_space
        """
        #normalize action space
        self.action_space = Box(low = np.array([-1,-1,-1]), #lower bounds for vel_x,vel_y,omega
                                     high=np.array([1,1,1])) #upper bounds for vel_x,vel_y,omega

        self.observation_space = Box(low = np.array([-6.58,-4.63,-math.pi]), #lower bounds of state
                                      high=np.array([6.58,4.63,math.pi]),dtype=np.float64) #upper bounds of state
        self.epi_len = 2500


        #states
        self.x = -3.19
        self.y = 3
        self.theta = 0
        self.state = np.array([self.x,self.y,self.theta])

        #RL constants
        self.ep_t = 0

        self.time_int = 25/self.epi_len
        # self.t2 = self.timestep*self.time_int
        #reference trajectory
        ref_trajectory=[[-1.5+5.8*math.cos(0.24*self.time_int*t+1.5), 5.8*math.sin(0.24*t*self.time_int+1.5)] for t in range(self.epi_len)]
        self.state_d = np.array(ref_trajectory)

        #Funnel for soft constraint currently values are according to paper
        l = np.array([0.7,0.7])
        ini_width = 0.2 #higher the value more is the initial funnel width
        rho_f = np.array([0.2,0.2])
        rho_0 = np.array(abs(np.array([self.x,self.y])- self.state_d[0,:]) + ini_width )
        gamma = np.array([(rho_0 - rho_f)*np.exp(-l*self.time_int*t) + rho_f for t in range(self.epi_len)])
        self.lb_soft = self.state_d - gamma
        self.ub_soft = self.state_d + gamma
        self.phi_ini_L = [0,0]
        self.phi_ini_U = [0,0]

        #creating final funnel
        mu = np.array([3,3])
        kc = np.array([3,3])
        self.lb_hard = np.array([-6.58,-4.63])
        self.ub_hard = np.array([6.58,4.63])
        t1 = np.linspace(0,self.time_int)
        self.phi_L,self.phi_U,self.Lb,self.Ub = [],[],[],[]
        self.j = []
        for i in range(self.epi_len):
            phi_Lo = odeint(self.bound, self.phi_ini_L, t1, args=(self.lb_soft[i,:], self.ub_hard, mu, kc))
            phi_sol_L = np.abs(phi_Lo[-1]) #this condition to be checked i.e,psi(modification signal) is always positive
            phi_U = odeint(self.bound, self.phi_ini_U, t1, args=(self.lb_hard, self.ub_soft[i,:], mu, kc))
            phi_sol_U = np.abs(phi_U[-1]) #this condition to be checked i.e,psi(modification signal) is always positive
            v = 10
            lower_bo = np.log(np.exp(v * (self.lb_soft[i,:] - phi_sol_L)) + np.exp(v * self.lb_hard)) / v
            upper_bo = -np.log(np.exp(-v * (self.ub_soft[i,:] + phi_sol_U)) + np.exp(-v * self.ub_hard)) / v
            self.phi_ini_L = phi_sol_L #will change according to comment in line 55
            self.phi_ini_U = phi_sol_U
            # self.phi_L.append(phi_sol_L)
            # self.phi_U.append(phi_sol_U)
            self.Lb.append(lower_bo)
            self.Ub.append(upper_bo)
            self.j.append(i)
        # print(self.Lb)
        low_x = [inner_list[0] for inner_list in self.Lb]
        low_y = [inner_list[1] for inner_list in self.Lb]
        high_x = [inner_list[0] for inner_list in self.Ub]
        high_y = [inner_list[1] for inner_list in self.Ub]
        plt.plot(self.j,low_x,self.j,high_x)
        plt.plot(self.j,low_y,self.j,high_y)
    def bound(self,phi,t,lb, ub, mu, kc): #####why t???
        eta = ub-lb
        dphi_dt = 0.5*(1-np.sign(eta-mu))*(1/(eta+phi))-kc*phi
        return dphi_dt



    def step(self, action):

        #flag to check if episode is complete or not
        done = False


        #get new position
        vel_x,vel_y,omega = 0.22*action[0], 0.22*action[1], 2.84*action[2]
        x_old,y_old,theta_old = self.x,self.y,self.theta

        self.x = x_old + vel_x * self.time_int
        self.y = y_old + vel_y * self.time_int
        self.theta = theta_old + omega * self.time_int
        self.state = np.array([self.x,self.y,self.theta])
        # To keep the angle theta between -pi to pi
        if(self.theta > math.pi or self.theta < -math.pi ):
           self.theta = ((self.theta + math.pi)%(2*math.pi))-math.pi

        #check if new position is within hard constraint
        if self.lb_hard[0] <= self.x <= self.ub_hard[0] and self.lb_hard[1] <= self.y <= self.ub_hard[1] :
            Rew_max = 100
            #check if within funnel and reward accordingly
            x_min,y_min = self.Lb[self.ep_t]
            x_max,y_max = self.Ub[self.ep_t]
            robust1 = Rew_max - ((self.x - (x_min + x_max)/2)**2)*(4*Rew_max/((x_max-x_min)**2))
            robust2 = Rew_max - ((self.y - (y_min + y_max)/2)**2)*(4*Rew_max/((y_max-y_min)**2))
            reward = np.clip(min(robust1, robust2), -10,Rew_max)

        else:
            #terminate the episode or restart the episode?
            reward = -100
            done = True

        self.ep_t +=1

        if self.ep_t == self.epi_len:
            done = True

        info = {}
        truncated = done
        return self.state, reward, done, truncated, info

    def render(self, mode="human"):
        pass

    def reset(self,seed = None,options=None):

        self.x = -3.19
        self.y = 3
        self.theta = 0
        self.state = np.array([self.x,self.y,self.theta])

        self.ep_t = 0

        info={}

        return self.state,info

    def  close(self):
        pass

    def seed(self, seed=None):
        pass

env = Robot_env.RobotEnv()

!pip install "stable-baselines3[extra]>=2.0.0a4"

from stable_baselines3.common.env_checker import check_env
# import Robot_env
env = RobotEnv()
# It will check your custom environment and output additional warnings if needed
check_env(env)

# import Robot_env
import numpy as np
import matplotlib as plt
from stable_baselines3 import PPO, A2C, DQN
from stable_baselines3.common.env_util import make_vec_env
from stable_baselines3.common.evaluation import evaluate_policy
# Instantiate the env
#vec_env = make_vec_env(Robot_env.RobotEnv, n_envs=1)
env = RobotEnv()
model = PPO("MlpPolicy",env,verbose=0)

best = -100

plot_reward = []
for i in range(400):
  model.learn(total_timesteps=2500)
  temp = evaluate_policy(model,model.env,n_eval_episodes=5)
  print("reward", temp, flush = True)
  plot_reward.append(temp)
  if(temp[0]>best):
    model.save("./model_best/omnirobot")
    best = temp[0]
  if(i%10==0):
    model.save("./model/omnirobot")

np.save('omnirobot.npy',np.array(plot_reward))


model.save("./model/omnirobot")

# import Robot_env
import numpy as np
import matplotlib as plt
from stable_baselines3 import PPO, A2C, DQN
from stable_baselines3.common.env_util import make_vec_env
from stable_baselines3.common.evaluation import evaluate_policy
# Instantiate the env
#vec_env = make_vec_env(Robot_env.RobotEnv, n_envs=1)
env = RobotEnv()
model = PPO("MlpPolicy",env,verbose=0)

best = -100

plot_reward = []
for i in range(400):
  model.learn(total_timesteps=5000)
  temp = evaluate_policy(model,model.env,n_eval_episodes=5)
  print("reward", temp, flush = True)
  plot_reward.append(temp)
  if(temp[0]>best):
    model.save("./model_best/omnirobot")
    best = temp[0]
  if(i%10==0):
    model.save("./model/omnirobot")

np.save('omnirobot.npy',np.array(plot_reward))


model.save("./model/omnirobot")

import torch

torch.tensor()